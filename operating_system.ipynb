{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/not-part-of-solution/Starving_cats/blob/main/operating_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQHZxecapAG1"
      },
      "source": [
        "По проекту:\n",
        "Проект похожий на наш по описанию, но только для птиц(https://github.com/pvburkov/Smart-Trough?ysclid=m782i981sl102115203)\n",
        "Полезные ссылочки:\n",
        "https://habr.com/ru/articles/322520/\n",
        "https://habr.com/ru/companies/ruvds/articles/558434/\n",
        "https://developer.android.com/kotlin?hl=ru\n",
        "Успешно найдена платформа кормушки с доступом к api-серверу, при желании можем прям подключится, надо только прочитать их требования(но раз разместили, значит не против)\n",
        "https://github.com/hasscc/catlink/blob/main/CONTRIBUTE.md //гитхаб милых чувачков, возможно и код толковый есть где-то.\n",
        "Если пишем распознавание надо:\n",
        "1. базова создаем свой фотосет из котов(Женины, мой, Аринин, максимального количества знакомых чтобы было больше разнообразия)\n",
        "2. пишем и обучаем собственную модель(теоретически думаю можем попытаться на питончике развернуть и с этим поиграться)\n",
        "3. подумать над предобработкой данных для загрузки в базу(https://habr.com/ru/companies/data_light/articles/857142/)\n",
        "4. тратим время на обучение модельки\n",
        "5. пытаемся жестко понять на чем нам нужно приложение, скорее всего нужен андроид, так что в руки изучать Kotlin\n",
        "Если что спокойно можем совмещать модель на питоне и приложение на Kotlin\n",
        "\n",
        "\n",
        "По поводу приложения и внутренних настроек:\n",
        "чекать время кормления и запоминать его(контрить чтобы жопки не ели больше еды чем надо и в неправильное время, условно говоря запоминатьб когда конкретная морда поела и следующий прием пищи )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTCbCvZIo6Is"
      },
      "source": [
        "Неделя 1: Подготовка данных и начальная разработка модели (24 февраля - 9 марта)\n",
        "Задачи:\n",
        "\n",
        "***Сбор изображений*** (24 - 26 февраля):\n",
        "\n",
        "Заключите соглашения с друзьями и знакомыми, чтобы они предоставили фотографии своих котов (обязательно сообщите количество требуемых фото).\n",
        "Создайте групповую беседу или используйте социальные сети для сбора изображений.\n",
        "\n",
        "***Разметка данных*** (27 - 28 февраля):\n",
        "\n",
        "Создайте структуру папок для хранения изображений, например: /cats/jena, /cats/ari, и т.д.\n",
        "Разработайте и сохраните CSV-файл или JSON-файл с метками (имена файлов и соответствующие метки котов).\n",
        "\n",
        "***Предобработка данных*** (1 - 5 марта):\n",
        "\n",
        "Измените размер изображений до нужного формата (например, 224x224) с помощью библиотеки Pillow или OpenCV.\n",
        "Реализуйте нормализацию изображений (деление значений на 255).\n",
        "Исследуйте и добавьте аугментацию данных (повороты, изменения контраста и яркости) с помощью Keras или imgaug.\n",
        "\n",
        "***Начало разработки модели*** (6 - 9 марта):\n",
        "\n",
        "Выберите библиотеку (TensorFlow или PyTorch), установите её и изучите основную документацию.\n",
        "Определите архитектуру модели (например, ResNet или VGG) и начните писать код для базовой реализации.\n",
        "\n",
        "Неделя 2: Завершение написания модели и обучение (10 - 16 марта)\n",
        "Задачи:\n",
        "\n",
        "***Завершение разработки кода модели*** (10-11 марта):\n",
        "\n",
        "Завершите построение и реализацию архитектуры модели. Проверьте её работоспособность на небольшом наборе данных.\n",
        "***Разделение данных*** (12-13 марта):\n",
        "\n",
        "Разделите данные на обучающую (70%), валидационную (15%) и тестовую выборки (15%) с помощью sklearn.model_selection.train_test_split.\n",
        "\n",
        "Запуск обучения модели (14-16 марта):\n",
        "\n",
        "Запустите процесс обучения, настройте гиперпараметры (скорость обучения, количество эпох и т.д.).\n",
        "\n",
        "Запишите метрики производительности (например, точность) с использованием TensorBoard для визуализации результатов.\n",
        "\n",
        "Неделя 3: Оценка модели и деплой (17 - 23 марта)\n",
        "Задачи:\n",
        "\n",
        "Оценка производительности модели (17 марта):\n",
        "\n",
        "Проверьте модель на тестовой выборке, соберите метрики (точность, полнота, F1-score) и создайте матрицу замешивания.\n",
        "Анализ и улучшение модели (18-20 марта):\n",
        "\n",
        "Проведите анализ ошибок и попробуйте улучшить производительность путем дообучения модели или изменения гиперпараметров.\n",
        "Деплой модели (21-23 марта):\n",
        "\n",
        "Подготовьте модель для развертывания. Сохраните её в формате, подходящем для использования, например, с помощью TensorFlow Serving или PyTorch.\n",
        "Неделя 4: Создание API и разработка Android-приложения (24 - 30 марта)\n",
        "Задачи:\n",
        "\n",
        "Создание API для модели (24-26 марта):\n",
        "\n",
        "Напишите сервер API с помощью Flask или FastAPI, обеспечив возможность загрузки изображений и получения предсказаний от модели.\n",
        "Протестируйте API на локальном сервере.\n",
        "Начало разработки Android-приложения (27-30 марта):\n",
        "\n",
        "Установите Android Studio, создайте новый проект и начните разработку интерфейса приложения для загрузки изображений и отображения результатов распознавания.\n",
        "Неделя 5: Завершение разработки Android-приложения и интеграция с API (31 марта - 6 апреля)\n",
        "Задачи:\n",
        "\n",
        "Разработка интерфейса приложения (31 марта - 2 апреля):\n",
        "\n",
        "Создайте основные экраны приложения: экран загрузки изображения, экран результата распознавания.\n",
        "Реализуйте базовые функции, такие как выбор изображения из галереи и кнопка для отправки на сервер.\n",
        "Интеграция API с приложением (3-5 апреля):\n",
        "\n",
        "Добавьте реализацию запросов к вашему API для загрузки изображений и получения предсказаний.\n",
        "Обработайте ответы API и отобразите результаты на экране.\n",
        "Тестирование приложения (6 апреля):\n",
        "\n",
        "Проверьте приложение на физическом устройстве или эмуляторе, убедитесь, что все функции работают корректно.\n",
        "Исправьте найденные ошибки, если они возникли.\n",
        "Неделя 6: Завершение проекта и отчет (7 - 13 апреля)\n",
        "Задачи:\n",
        "\n",
        "Финальная доработка (7-8 апреля):\n",
        "\n",
        "Проработайте приложение, добавив дополнительные улучшения на основе обратной связи.\n",
        "Убедитесь, что приложение полностью функционально и стабильно.\n",
        "Документация и отчет (9-10 апреля):\n",
        "\n",
        "Подготовьте отчет о ходе проекта, включая все этапы, результаты и выводы.\n",
        "Оформите документацию для API и Android-приложения.\n",
        "Презентация проекта (11-13 апреля):\n",
        "\n",
        "Подготовьте материалы для представления проекта: слайды, демонстрация приложения, результаты работы модели и т.д.\n",
        "Если возможно, организуйте встречу с заинтересованными сторонами для обсуждения результатов.\n",
        "Неделя 7: Обратная связь и доработка (14 - 20 апреля)\n",
        "Задачи:\n",
        "\n",
        "Сбор обратной связи (14-16 апреля):\n",
        "\n",
        "Предоставьте доступ к приложению и API вашим друзьям или знакомым, соберите мнения о функциональности и производительности.\n",
        "Определитесь с улучшениями, которые можно внести на основе полученных отзывов.\n",
        "Внедрение улучшений (17-20 апреля):\n",
        "\n",
        "Реализуйте запланированные улучшения на основе обратной связи, протестируйте изменения и внесите окончательные правки перед публикацией.\n",
        "Завершение проекта (до 4 мая)\n",
        "Публикация приложения (до 4 мая):\n",
        "Если вы планируете публиковать приложение в Google Play, подготовьте его к публикации, создайте описание, иконки и снимки экрана.\n",
        "Убедитесь, что соблюдены все требования для публикации в магазине."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mqirOcMSxUBE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84EOA2Hzs0VV"
      },
      "source": [
        "Первое что делаем - качаем на PyCharm библиотеки\n",
        "\n",
        "pip install tensorflow numpy pandas matplotlib opencv-python\n",
        "\n",
        "pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0AdYHWUhJkXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06f42ff-7a7e-4ac1-d7aa-180aff19f49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3u5vrkq5O1g",
        "outputId": "a05986d8-8028-4c22-acfd-370b48e88501",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 380 images belonging to 3 classes.\n",
            "Found 219 images belonging to 3 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 14s/step - accuracy: 0.1671 - loss: 1.6887 - val_accuracy: 0.2831 - val_loss: 1.4031\n",
            "Epoch 2/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - accuracy: 0.1581 - loss: 1.6742 - val_accuracy: 0.3059 - val_loss: 1.3371\n",
            "Epoch 3/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step - accuracy: 0.2128 - loss: 1.5228 - val_accuracy: 0.3288 - val_loss: 1.2794\n",
            "Epoch 4/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step - accuracy: 0.1755 - loss: 1.5175 - val_accuracy: 0.3562 - val_loss: 1.2288\n",
            "Epoch 5/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 3s/step - accuracy: 0.2332 - loss: 1.4570 - val_accuracy: 0.3790 - val_loss: 1.1797\n",
            "Epoch 6/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step - accuracy: 0.2028 - loss: 1.4717 - val_accuracy: 0.3927 - val_loss: 1.1327\n",
            "Epoch 7/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3s/step - accuracy: 0.2149 - loss: 1.4413 - val_accuracy: 0.4292 - val_loss: 1.0893\n",
            "Epoch 8/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 4s/step - accuracy: 0.2621 - loss: 1.3852 - val_accuracy: 0.4475 - val_loss: 1.0533\n",
            "Epoch 9/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 4s/step - accuracy: 0.2902 - loss: 1.3368 - val_accuracy: 0.4612 - val_loss: 1.0164\n",
            "Epoch 10/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 4s/step - accuracy: 0.3237 - loss: 1.3517 - val_accuracy: 0.4840 - val_loss: 0.9871\n",
            "Epoch 1/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 4s/step - accuracy: 0.4340 - loss: 1.0642 - val_accuracy: 0.5799 - val_loss: 0.8683\n",
            "Epoch 2/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3s/step - accuracy: 0.6945 - loss: 0.7724 - val_accuracy: 0.6347 - val_loss: 0.7706\n",
            "Epoch 3/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3s/step - accuracy: 0.7765 - loss: 0.6657 - val_accuracy: 0.7808 - val_loss: 0.6754\n",
            "Epoch 4/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 3s/step - accuracy: 0.8393 - loss: 0.5536 - val_accuracy: 0.8539 - val_loss: 0.5893\n",
            "Epoch 5/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step - accuracy: 0.8289 - loss: 0.5327 - val_accuracy: 0.9041 - val_loss: 0.5151\n",
            "Epoch 6/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3s/step - accuracy: 0.8876 - loss: 0.4294 - val_accuracy: 0.9269 - val_loss: 0.4549\n",
            "Epoch 7/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4s/step - accuracy: 0.9543 - loss: 0.3417 - val_accuracy: 0.9361 - val_loss: 0.4062\n",
            "Epoch 8/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 3s/step - accuracy: 0.9303 - loss: 0.3258 - val_accuracy: 0.9361 - val_loss: 0.3656\n",
            "Epoch 9/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 3s/step - accuracy: 0.9316 - loss: 0.3036 - val_accuracy: 0.9406 - val_loss: 0.3380\n",
            "Epoch 10/10\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 3s/step - accuracy: 0.9297 - loss: 0.3165 - val_accuracy: 0.9543 - val_loss: 0.3125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Параметры для обработки изображений\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "# Генераторы данных для обучения и валидации\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.4,\n",
        "    height_shift_range=0.4,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "# Генератор для обучающих данных\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/operating_system/learning/',\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Генератор для валидационных данных\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/operating_system/control/',\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Создание модели\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Замораживание базовой модели\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Коллбек для досрочной остановки\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Обучение модели\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Оттаивание базовой модели и повторное обучение\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "             loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Сохранение модели\n",
        "model.save('/content/drive/MyDrive/operating_system/cat_recognition_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AuAKDrje7ZL",
        "outputId": "50e96cae-67ce-49e5-820f-b4973b4bea42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Предсказанная метка: Ева\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Не удалось открыть или найти изображение: {image_path}\")\n",
        "\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "\n",
        "    img = img.astype('float32') / 255.0\n",
        "\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "\n",
        "# Загрузка обученной модели\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/operating_system/cat_recognition_model.h5')\n",
        "\n",
        "# Предобработка нового изображения\n",
        "image_path = '/content/drive/MyDrive/operating_system/19_45.jpg'\n",
        "preprocessed_image = preprocess_image(image_path)\n",
        "\n",
        "# Прогнозирование\n",
        "predictions = model.predict(preprocessed_image)\n",
        "predicted_class = np.argmax(predictions[0])\n",
        "# Отображение результата\n",
        "class_labels = train_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}  # Разворачиваем словарь\n",
        "predicted_label = class_labels[predicted_class]\n",
        "\n",
        "print(f'Предсказанная метка: {predicted_label}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}